{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capiAdmin\\anaconda3\\envs\\dqn_rlsel\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from model_set import Custom1DCNN\n",
    "import capilab_dataset2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "label_keys = {'ball':1,'box':2,'pen':3,'no':0}\n",
    "\n",
    "X, y = capilab_dataset2.get(['Datasets\\Lai_JulyData.mat', 'Datasets\\Lai_JulyData.mat', 'Datasets\\Suguro_JulyData.mat', 'Datasets\\Takahashi_JulyData.mat']) \n",
    "\n",
    "try:\n",
    "    _x, x_test,  _y, y_test = train_test_split(X, y, test_size = .1, stratify = y, random_state = 420)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(_x, _y, test_size = .2, stratify = _y, random_state = 420)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "ckpt = ModelCheckpoint('tkhDatasetTest/',monitor='val_loss', verbose=False,\n",
    "                                    save_best_only=True, save_weights_only=False, mode='auto',save_freq=\"epoch\")\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, restore_best_weights=True, verbose = False)\n",
    "cb_list = [ckpt, es]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STFU\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP I\n",
    "- Dataset:********* capilab_dataset2\n",
    "- Classifier:****** Custom1DCNN\n",
    "- Preprocessor:**** None\n",
    "- Channel:********* ALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting the result>:   0%|                                                   | 0/10 [00:00<?, ?it/s]WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
      "getting the result>:  80%|██████████████████████████████████▍        | 8/10 [13:03<03:13, 96.54s/it]WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
      "getting the result>: 100%|██████████████████████████████████████████| 10/10 [16:31<00:00, 99.12s/it]\n",
      "  0%|          | 0/10 [16:31<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L|ACC and STD L|ACC = 0.8619410812854766|0.6798611164093018 0.038156608795933335|0.01972488408407706 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_x_train = x_train.copy()\n",
    "_x_test = x_test.copy()\n",
    "_x_val = x_val.copy()\n",
    "\n",
    "vl = []\n",
    "va = []\n",
    "\n",
    "\n",
    "with tqdm(total = 10, position = 0, leave = True) as pbar:\n",
    "    for e in tqdm(range(10), ncols = 100, position = 0, leave = True, desc =\"getting the result>\"):\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        model = Custom1DCNN(inp_shape=(_x_train.shape[1], _x_train.shape[2]), output_classes=y_train.shape[1])\n",
    "        model.compile(optimizer = opt, loss=loss , metrics=['accuracy'])\n",
    "\n",
    "        hist = model.fit(_x_train, y_train, batch_size=16, epochs = 70, verbose = False, validation_data = (_x_val,y_val), callbacks = cb_list)\n",
    "\n",
    "        acc = hist.history['accuracy']\n",
    "        val_acc = hist.history['val_accuracy']\n",
    "        loss = hist.history['loss']\n",
    "        val_loss = hist.history['val_loss']\n",
    "        #eval\n",
    "        vloss, vacc =  model.evaluate(_x_val, y_val, verbose = False)\n",
    "        va.append(vacc)\n",
    "        vl.append(vloss)\n",
    "\n",
    "print(\"Mean L|ACC and STD L|ACC = {}|{} {}|{} \".format(\n",
    "                                                        np.mean(np.array(vl)),\n",
    "                                                        np.mean(np.array(va)),\n",
    "                                                        np.std(np.array(vl)),\n",
    "                                                        np.std(np.array(va))\n",
    "                                                    )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# yPred = model.predict(x_test)\n",
    "# yTestClass = np.argmax(y_test, axis=1)\n",
    "# yPredClass = np.argmax(yPred,axis=1)\n",
    "\n",
    "# report =  classification_report(yTestClass, yPredClass, target_names=[\"No Move\", \"Ball\", 'Box / Mobile', 'Pen'])\n",
    "# cf_matrix = confusion_matrix(yTestClass,yPredClass)\n",
    "# print('\\n----------<Classification report>----------\\n\\n',report)\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(10,10)})\n",
    "# sns.set(font_scale=2)\n",
    "# xticks = yticks = ['NM', 'BL', 'PX', 'PN']\n",
    "# sns.heatmap(cf_matrix, annot=True, xticklabels = xticks, yticklabels=yticks, cmap='gray', linewidth=.5)\n",
    "\n",
    "# plt.figure(figsize=(20,7))\n",
    "# sns.set(font_scale=1)\n",
    "# plt.subplot(121), plt.plot(acc, label='acc'),plt.plot(val_acc,label='val_acc'),plt.title(\"Accuracy VS Validation Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.subplot(122), plt.plot(loss, label='loss'),plt.plot(val_loss,label='val_loss'),plt.title(\"Loss VS Validation Loss\")\n",
    "# plt.legend()\n",
    "# plt.suptitle('Experiment 1', fontsize = 20)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP II\n",
    "- Dataset:********* capilab_dataset2\n",
    "- Classifier:****** Custom1DCNN\n",
    "- Preprocessor:**** None\n",
    "- Channel:********* REMOVE -> ['C4','F4', 'Fp2','Fp1','F3', 'O1', 'F7', 'F8', 'Fz']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting the result>: 100%|██████████████████████████████████████████| 10/10 [14:10<00:00, 85.07s/it]\n",
      "  0%|          | 0/10 [14:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L|ACC and STD L|ACC = 0.9791095137596131|0.5958333283662796 0.13865926792012134|0.1125685664355553 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_channel_map = {0: 'F4', 1: 'C4', 2: 'Pa', 3: 'Cz', 4: 'F3', 5: 'C3', 6: 'P3', 7: 'F7', 8: 'T3', \n",
    "                                    9: 'T5', 10: 'Fp1', 11: 'Fp2', 12: 'T4', 13: 'F8', 14: 'Fz', 15: 'Pz', 16: 'T6', 17: 'O2', 18: 'O1'}\n",
    "\n",
    "removed_ch = ['C4','F4', 'Fp2','Fp1','F3', 'O1', 'F7', 'F8', 'Fz']\n",
    "target = [k for k,v in dataset_channel_map.items() if v not in removed_ch]\n",
    "\n",
    "_x_train = x_train[:,:,target].copy()\n",
    "_x_test = x_test[:,:,target].copy()\n",
    "_x_val = x_val[:,:,target].copy()\n",
    "\n",
    "vl = []\n",
    "va = []\n",
    "\n",
    "with tqdm(total = 10, position = 0, leave = True) as pbar:\n",
    "    for e in tqdm(range(10), ncols = 100, position = 0, leave = True, desc =\"getting the result>\"):\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        model = Custom1DCNN(inp_shape=(_x_train.shape[1], _x_train.shape[2]), output_classes=y_train.shape[1])\n",
    "        model.compile(optimizer = opt, loss=loss , metrics=['accuracy'])\n",
    "\n",
    "        hist = model.fit(_x_train, y_train, batch_size=16, epochs = 70, verbose = False, validation_data = (_x_val,y_val), callbacks = cb_list)\n",
    "\n",
    "        acc = hist.history['accuracy']\n",
    "        val_acc = hist.history['val_accuracy']\n",
    "        loss = hist.history['loss']\n",
    "        val_loss = hist.history['val_loss']\n",
    "        #eval\n",
    "        vloss, vacc =  model.evaluate(_x_val, y_val, verbose = False)\n",
    "        va.append(vacc)\n",
    "        vl.append(vloss)\n",
    "\n",
    "print(\"Mean L|ACC and STD L|ACC = {}|{} {}|{} \".format(\n",
    "                                                        np.mean(np.array(vl)),\n",
    "                                                        np.mean(np.array(va)),\n",
    "                                                        np.std(np.array(vl)),\n",
    "                                                        np.std(np.array(va))\n",
    "                                                    )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP III\n",
    "- Dataset:********* capilab_dataset2\n",
    "- Classifier:****** Custom1DCNN\n",
    "- Preprocessor:**** None\n",
    "- Channel:********* BEST_CH = (C3, C4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting the result>: 100%|██████████████████████████████████████████| 10/10 [12:38<00:00, 75.81s/it]\n",
      "  0%|          | 0/10 [12:38<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L|ACC and STD L|ACC = 1.103079915046692|0.5453703761100769 0.09630244746984863|0.09530292340334227 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "vl = []\n",
    "va = []\n",
    "\n",
    "dataset_channel_map = {0: 'F4', 1: 'C4', 2: 'Pa', 3: 'Cz', 4: 'F3', 5: 'C3', 6: 'P3', 7: 'F7', 8: 'T3', \n",
    "                                    9: 'T5', 10: 'Fp1', 11: 'Fp2', 12: 'T4', 13: 'F8', 14: 'Fz', 15: 'Pz', 16: 'T6', 17: 'O2', 18: 'O1'}\n",
    "\n",
    "target = [1, 5]\n",
    "_x_train = x_train[:,:,target].copy()\n",
    "_x_test = x_test[:,:,target].copy()\n",
    "_x_val = x_val[:,:,target].copy()\n",
    "\n",
    "\n",
    "with tqdm(total = 10, position = 0, leave = True) as pbar:\n",
    "    for e in tqdm(range(10), ncols = 100, position = 0, leave = True, desc =\"getting the result>\"):\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        model = Custom1DCNN(inp_shape=(_x_train.shape[1], _x_train.shape[2]), output_classes=y_train.shape[1])\n",
    "        model.compile(optimizer = opt, loss=loss , metrics=['accuracy'])\n",
    "\n",
    "        hist = model.fit(_x_train, y_train, batch_size=16, epochs = 70, verbose = False, validation_data = (_x_val,y_val), callbacks = cb_list)\n",
    "\n",
    "        acc = hist.history['accuracy']\n",
    "        val_acc = hist.history['val_accuracy']\n",
    "        loss = hist.history['loss']\n",
    "        val_loss = hist.history['val_loss']\n",
    "        #eval\n",
    "        vloss, vacc =  model.evaluate(_x_val, y_val, verbose = False)\n",
    "        va.append(vacc)\n",
    "        vl.append(vloss)\n",
    "\n",
    "print(\"Mean L|ACC and STD L|ACC = {}|{} {}|{} \".format(\n",
    "                                                        np.mean(np.array(vl)),\n",
    "                                                        np.mean(np.array(va)),\n",
    "                                                        np.std(np.array(vl)),\n",
    "                                                        np.std(np.array(va))\n",
    "                                                    )\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP IV\n",
    "- Dataset:********* capilab_dataset2\n",
    "- Classifier:****** Custom1DCNN\n",
    "- Preprocessor:**** None\n",
    "- Channel:********* SINGLE = (C4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting the result>: 100%|██████████████████████████████████████████| 10/10 [12:36<00:00, 75.68s/it]\n",
      "  0%|          | 0/10 [12:36<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L|ACC and STD L|ACC = 1.1097235202789306|0.5513888895511627 0.03194464242263654|0.02520491640029468 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "vl = []\n",
    "va = []\n",
    "\n",
    "dataset_channel_map = {0: 'F4', 1: 'C4', 2: 'Pa', 3: 'Cz', 4: 'F3', 5: 'C3', 6: 'P3', 7: 'F7', 8: 'T3', \n",
    "                                    9: 'T5', 10: 'Fp1', 11: 'Fp2', 12: 'T4', 13: 'F8', 14: 'Fz', 15: 'Pz', 16: 'T6', 17: 'O2', 18: 'O1'}\n",
    "\n",
    "target = [1]\n",
    "_x_train = x_train[:,:,target].copy()\n",
    "_x_test = x_test[:,:,target].copy()\n",
    "_x_val = x_val[:,:,target].copy()\n",
    "\n",
    "\n",
    "with tqdm(total = 10, position = 0, leave = True) as pbar:\n",
    "    for e in tqdm(range(10), ncols = 100, position = 0, leave = True, desc =\"getting the result>\"):\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        model = Custom1DCNN(inp_shape=(_x_train.shape[1], _x_train.shape[2]), output_classes=y_train.shape[1])\n",
    "        model.compile(optimizer = opt, loss=loss , metrics=['accuracy'])\n",
    "\n",
    "        hist = model.fit(_x_train, y_train, batch_size=16, epochs = 70, verbose = False, validation_data = (_x_val,y_val), callbacks = cb_list)\n",
    "\n",
    "        acc = hist.history['accuracy']\n",
    "        val_acc = hist.history['val_accuracy']\n",
    "        loss = hist.history['loss']\n",
    "        val_loss = hist.history['val_loss']\n",
    "        #eval\n",
    "        vloss, vacc =  model.evaluate(_x_val, y_val, verbose = False)\n",
    "        va.append(vacc)\n",
    "        vl.append(vloss)\n",
    "\n",
    "print(\"Mean L|ACC and STD L|ACC = {}|{} {}|{} \".format(\n",
    "                                                        np.mean(np.array(vl)),\n",
    "                                                        np.mean(np.array(va)),\n",
    "                                                        np.std(np.array(vl)),\n",
    "                                                        np.std(np.array(va))\n",
    "                                                    )\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Channel Number        | Mean Validation Accuracy (std)    | Mean Validation Loss (std)    |\n",
    "| ---                   | ---                               | ---                           |\n",
    "| 1 ch (C4)             |  0.551388 (0.0252)                | 1.109723 (0.0319)             |\n",
    "| 2 ch (C3, C4)         |  0.545370 (0.0953)                | 1.103079 (0.096)              |\n",
    "| 10 ch remove blink    |  0.598533 (0.1125)                | 0.979109 (0.1387)             |\n",
    "| 19 ch all             |  0.679861 (0.0197)                | 0.861941 (0.0381)             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 2160\n  y sizes: 2400\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\capiAdmin\\Desktop\\WindowProj\\dqn_rlsel\\test_classifier.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/capiAdmin/Desktop/WindowProj/dqn_rlsel/test_classifier.ipynb#ch0000015?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mevaluate(_x, y)\n",
      "File \u001b[1;32mc:\\Users\\capiAdmin\\anaconda3\\envs\\dqn_rlsel\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\capiAdmin\\anaconda3\\envs\\dqn_rlsel\\lib\\site-packages\\keras\\engine\\data_adapter.py:1655\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1651\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1652\u001b[0m       label, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m   1653\u001b[0m                        \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)))\n\u001b[0;32m   1654\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1655\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 2160\n  y sizes: 2400\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dqn_rlsel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559ce9826e6417ab999de3ecc19512d1b857172853f4ae4f49a5d92401e6c36d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
