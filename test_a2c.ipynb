{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "import silence_tensorflow #うるさいしないために\n",
    "silence_tensorflow.silence_tensorflow()\n",
    "\n",
    "from a2c_agent import ActorCritic\n",
    "from eeg_env import EEGChannelOptimze\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import model_set\n",
    "import capilab_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n",
      "Creating an evnironment...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fname = ['Datasets/Lai_JulyData.mat', 'Datasets/Takahashi_JulyData.mat']\n",
    "dataset_channel_map = {'F4': 0, 'C4': 1, 'Pa': 2, 'Cz': 3, 'F3': 4, 'C3': 5, 'P3': 6, 'F7': 7, 'T3': 8, 'T5': 9, \n",
    "                           'Fp1': 10, 'Fp2': 11, 'T4': 12, 'F8': 13, 'Fz': 14, 'Pz': 15, 'T6': 16, 'O2': 17, 'O1': 18}\n",
    "\n",
    "Xx, yy = capilab_dataset2.get(fname)\n",
    "\n",
    "dataset_info = None\n",
    "try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "\n",
    "    X, x_test,  y, y_test = train_test_split(Xx, yy, test_size = .1, stratify = yy, random_state = 420)\n",
    "    dataset_info = {\n",
    "        \"X\":X,\n",
    "        \"y\":y,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test,\n",
    "        \"nbr_class\":y.shape[1],\n",
    "        \"data_shape\":(X.shape[1], X.shape[2]),\n",
    "        \"nbr_data\":X.shape[0],\n",
    "        \"ch_map\":dataset_channel_map\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "if dataset_info is None:\n",
    "    print(\"Error\")\n",
    "else:\n",
    "    print(\"Creating an evnironment...\")\n",
    "    env = EEGChannelOptimze(dataset_info, model_set.Custom1DCNN, 0.25)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "#Actor Critic Model\n",
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "#Episode Parameter\n",
    "min_episodes_criterion = 5\n",
    "max_episodes = 300\n",
    "max_steps_per_episode = 6\n",
    "\n",
    "reward_threshold = 0.75\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  return (state.astype(np.int32), np.array(reward, np.float32), np.array(done, np.bool))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], [tf.int32, tf.float32, tf.bool])\n",
    "\n",
    "\n",
    "def get_expected_return(rewards: tf.Tensor, gamma: float, standardize: bool = True) -> tf.Tensor:\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)  #place holder for expect return calculation\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns\n",
    "\n",
    "\n",
    "def compute_loss(action_probs: tf.Tensor,  values: tf.Tensor,  returns: tf.Tensor) -> tf.Tensor:\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss\n",
    "\n",
    "def run_episode(initial_state: tf.Tensor,  model: tf.keras.Model, max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "\n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state)\n",
    "    # Sample next action from the action probability distribution\n",
    "\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "\n",
    "    state, reward, done = tf_env_step(action)\n",
    "\n",
    "    state.set_shape(initial_state_shape)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "    # Apply action to the environment to get next state and reward\n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "\n",
    "  return action_probs, values, rewards\n",
    "\n",
    "@tf.function\n",
    "def train_step(initial_state: tf.Tensor, model: tf.keras.Model, gamma: float, max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(initial_state, model, max_steps_per_episode) \n",
    "    \n",
    "    # Calculate expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "    # Calculating loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runnning an episode with inital_state Tensor(\"initial_state:0\", shape=(19,), dtype=int32)\n",
      "Actor action logits Tensor(\"while/actor_critic_2/dense_7/BiasAdd:0\", shape=(1, 19), dtype=float32), and critic of value Tensor(\"while/actor_critic_2/dense_8/BiasAdd:0\", shape=(1, 1), dtype=float32)\n",
      "sampling an action log\n",
      "Select action Tensor(\"while/strided_slice:0\", shape=(), dtype=int64), prob distribute Tensor(\"while/Softmax:0\", shape=(1, 19), dtype=float32)\n",
      "Step..........\n",
      "state Tensor(\"while/PyFunc:0\", dtype=int32, device=/job:localhost/replica:0/task:0) | reward Tensor(\"while/PyFunc:1\", dtype=float32, device=/job:localhost/replica:0/task:0) | done Tensor(\"while/PyFunc:2\", dtype=bool, device=/job:localhost/replica:0/task:0)\n",
      "Done first td, calculate expected return\n",
      "Compute loss from both actor and critic\n",
      "Update actor/critic weigth\n",
      "\n",
      "STATE [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n",
      "\n",
      "STATE [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n",
      "\n",
      "STATE [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n",
      "\n",
      "STATE [1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n",
      "\n",
      "STATE [1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n",
      "\n",
      "STATE [1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [06:54<34:25:23, 414.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 episode with reward 0.06499999761581421\n",
      "\n",
      "STATE [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n",
      "\n",
      "STATE [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n",
      "Training on fold 5/10\n",
      "Training on fold 6/10\n",
      "Training on fold 7/10\n",
      "Training on fold 8/10\n",
      "Training on fold 9/10\n",
      "Training on fold 10/10\n",
      "\n",
      "STATE [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Training on fold 1/10\n",
      "Training on fold 2/10\n",
      "Training on fold 3/10\n",
      "Training on fold 4/10\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(env.reset(), dtype=tf.int32) #reset env\n",
    "        episode_reward = float(train_step(initial_state, model, gamma, max_steps_per_episode))\n",
    "        print(f\"Done 1 episode with reward {episode_reward}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] 0.0016666667 False\n"
     ]
    }
   ],
   "source": [
    "print(x, reward, done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('qlearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe6cb5332f7608343617e3647f5fb8d44ed789daf04b840340e4a8e07f78623d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
